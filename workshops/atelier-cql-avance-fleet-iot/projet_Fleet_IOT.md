Projet Ã  Rendre : SystÃ¨me Fleet Tracing IoT avec ETL et Dashboard
Contexte du Projet
Vous intÃ©grez une Ã©quipe Data/IoT chargÃ©e de dÃ©velopper une solution complÃ¨te de suivi de flotte en temps rÃ©el. Le projet comprend trois composants majeurs :

Pipeline ETL : Ingestion et transformation des donnÃ©es de tÃ©lÃ©mÃ©trie via Python
Cluster Cassandra Multi-Datacenter (MDC) : Stockage distribuÃ© haute performance
Dashboard Analytics : Visualisation temps rÃ©el et historique des donnÃ©es

Votre mission : concevoir, implÃ©menter et dÃ©ployer l'ensemble de la stack avec Docker Compose.

Architecture Globale du SystÃ¨me
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLEET TRACING PLATFORM                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   DATA SOURCES   â”‚
                    â”‚  (IoT Devices)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚         VOLET 1 : ETL            â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
          â”‚  â”‚   Python ETL Pipeline      â”‚  â”‚
          â”‚  â”‚  - cassandra-driver        â”‚  â”‚
          â”‚  â”‚  - pandas / data cleaning  â”‚  â”‚
          â”‚  â”‚  - batch / streaming       â”‚  â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    CASSANDRA MDC CLUSTER           â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
          â”‚  â”‚  DC1 (dc1)   â”‚  DC2 (dc2)   â”‚   â”‚
          â”‚  â”‚  - node1     â”‚  - node3     â”‚   â”‚
          â”‚  â”‚  - node2     â”‚  - node4     â”‚   â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
          â”‚  Replication: {'dc1': 2, 'dc2': 2} â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚      VOLET 2 : DASHBOARD         â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
          â”‚  â”‚   Streamlit / Dash / Flask â”‚  â”‚
          â”‚  â”‚  - Real-time monitoring    â”‚  â”‚
          â”‚  â”‚  - Historical analytics    â”‚  â”‚
          â”‚  â”‚  - Alerts visualization    â”‚  â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Objectifs PÃ©dagogiques
Volet 1 : ETL Python-Cassandra

MaÃ®triser le driver cassandra-driver (Python)
ImplÃ©menter des stratÃ©gies d'insertion (batch, async)
GÃ©rer la prÃ©paration des donnÃ©es (bucketing temporel, dÃ©-duplication)
Monitorer les performances (latence, throughput)

Volet 2 : Dashboard Analytics

Construire des visualisations temps rÃ©el Ã  partir de Cassandra
Optimiser les requÃªtes CQL pour l'analytique
ImplÃ©menter des agrÃ©gations cÃ´tÃ© application
GÃ©rer la pagination et les requÃªtes lourdes

Volet 3 : DÃ©ploiement Docker Compose

Orchestrer un cluster Cassandra multi-datacenter
Configurer la rÃ©plication inter-datacenter
GÃ©rer les dÃ©pendances entre services
ImplÃ©menter health checks et restart policies


Architecture Technique DÃ©taillÃ©e
ModÃ¨le de DonnÃ©es Cassandra
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   KEYSPACE: fleet_iot                       â”‚
â”‚   Replication: NetworkTopologyStrategy                      â”‚
â”‚   {'dc1': 2, 'dc2': 2}                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TABLE 1: devices_by_fleet
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PK: (fleet_id)                           â”‚
â”‚ CC: device_id                            â”‚
â”‚ Colonnes: model, activated_at, status    â”‚
â”‚ Usage: Liste des devices par flotte      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TABLE 2: latest_telemetry_by_device
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PK: (device_id)                          â”‚
â”‚ CC: -                                    â”‚
â”‚ Colonnes: last_ts, lat, lon, speed_kmh,  â”‚
â”‚          battery_pct, temp_c             â”‚
â”‚ Usage: Ã‰tat actuel (dashboard temps rÃ©el)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TABLE 3: telemetry_by_device_day
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PK: (device_id, day)                     â”‚
â”‚ CC: ts DESC                              â”‚
â”‚ Colonnes: lat, lon, speed_kmh,           â”‚
â”‚          battery_pct, temp_c, zone       â”‚
â”‚ Usage: Historique journalier             â”‚
â”‚ TTL: 30 jours                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TABLE 4: alerts_by_fleet_day
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PK: (fleet_id, day, severity)            â”‚
â”‚ CC: ts DESC, device_id                   â”‚
â”‚ Colonnes: alert_type, message, resolved  â”‚
â”‚ Usage: Alerting et monitoring            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TABLE 5: fleet_analytics_by_day
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PK: (fleet_id, day)                      â”‚
â”‚ CC: hour                                 â”‚
â”‚ Colonnes: total_distance_km,             â”‚
â”‚          avg_speed_kmh, alerts_count,    â”‚
â”‚          active_devices_count            â”‚
â”‚ Usage: AgrÃ©gations prÃ©-calculÃ©es         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Structure du Projet
fleet-tracing-project/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â”‚
â”œâ”€â”€ cassandra/
â”‚   â”œâ”€â”€ init-scripts/
â”‚   â”‚   â””â”€â”€ 01-create-schema.cql
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ cassandra.yaml (optionnel)
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ etl_pipeline.py
â”‚   â”œâ”€â”€ data_generator.py
â”‚   â”œâ”€â”€ cassandra_writer.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ realtime.py
â”‚   â”‚   â”œâ”€â”€ analytics.py
â”‚   â”‚   â””â”€â”€ alerts.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ cassandra_client.py
â”‚
â””â”€â”€ data/
    â””â”€â”€ sample_telemetry.csv

VOLET 1 : Pipeline ETL Python-Cassandra
Architecture du Pipeline ETL
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ETL PIPELINE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    EXTRACT                TRANSFORM              LOAD
       â”‚                       â”‚                    â”‚
       â–¼                       â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Sources â”‚      â”‚  Validation  â”‚     â”‚  Cassandra   â”‚
â”‚              â”‚â”€â”€â”€â”€â”€â–¶â”‚  Enrichment  â”‚â”€â”€â”€â”€â–¶â”‚   Writer     â”‚
â”‚ - CSV files  â”‚      â”‚  Bucketing   â”‚     â”‚              â”‚
â”‚ - IoT API    â”‚      â”‚  Aggregation â”‚     â”‚ - Batch      â”‚
â”‚ - Kafka      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ - Async      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚ - Prepared   â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Composants Ã  ImplÃ©menter
1.1 Configuration Cassandra (cassandra_writer.py)
pythonfrom cassandra.cluster import Cluster, ExecutionProfile
from cassandra.policies import DCAwareRoundRobinPolicy, TokenAwarePolicy
from cassandra.query import BatchStatement, PreparedStatement
from datetime import datetime, date
import logging

class CassandraWriter:
    def __init__(self, contact_points, keyspace, datacenter='dc1'):
        """
        Initialize Cassandra connection with MDC support
        
        Args:
            contact_points: List of Cassandra node IPs
            keyspace: Target keyspace
            datacenter: Preferred datacenter for reads
        """
        # Configuration de la politique de load balancing
        profile = ExecutionProfile(
            load_balancing_policy=TokenAwarePolicy(
                DCAwareRoundRobinPolicy(local_dc=datacenter)
            )
        )
        
        self.cluster = Cluster(
            contact_points=contact_points,
            execution_profiles={'default': profile},
            protocol_version=4
        )
        self.session = self.cluster.connect(keyspace)
        self.logger = logging.getLogger(__name__)
        
        # Prepared statements pour performance
        self._prepare_statements()
    
    def _prepare_statements(self):
        """Prepare CQL statements pour rÃ©utilisation"""
        self.insert_latest = self.session.prepare("""
            INSERT INTO latest_telemetry_by_device 
            (device_id, last_ts, lat, lon, speed_kmh, battery_pct, temp_c)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """)
        
        self.insert_history = self.session.prepare("""
            INSERT INTO telemetry_by_device_day 
            (device_id, day, ts, lat, lon, speed_kmh, battery_pct, temp_c, zone)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            USING TTL 2592000
        """)
        
        self.insert_alert = self.session.prepare("""
            INSERT INTO alerts_by_fleet_day 
            (fleet_id, day, severity, ts, device_id, alert_type, message)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """)
    
    def write_telemetry_batch(self, telemetry_records):
        """
        Ã‰criture par batch de donnÃ©es de tÃ©lÃ©mÃ©trie
        
        Args:
            telemetry_records: List de dicts avec les donnÃ©es
        """
        batch = BatchStatement()
        
        for record in telemetry_records:
            device_id = record['device_id']
            ts = record['timestamp']
            day = ts.date()
            
            # Latest state (UPSERT automatique)
            batch.add(self.insert_latest, (
                device_id, ts, 
                record['lat'], record['lon'],
                record['speed_kmh'], record['battery_pct'],
                record['temp_c']
            ))
            
            # Historical data avec bucketing journalier
            batch.add(self.insert_history, (
                device_id, day, ts,
                record['lat'], record['lon'],
                record['speed_kmh'], record['battery_pct'],
                record['temp_c'], record.get('zone', 'unknown')
            ))
        
        try:
            self.session.execute(batch)
            self.logger.info(f"Batch de {len(telemetry_records)} records insÃ©rÃ©")
        except Exception as e:
            self.logger.error(f"Erreur batch insert: {e}")
            raise
    
    def write_telemetry_async(self, telemetry_records):
        """
        Ã‰criture asynchrone pour haute performance
        """
        futures = []
        
        for record in telemetry_records:
            # Insertion asynchrone
            future = self.session.execute_async(
                self.insert_latest,
                (record['device_id'], record['timestamp'], ...)
            )
            futures.append(future)
        
        # Attendre toutes les insertions
        for future in futures:
            try:
                future.result()
            except Exception as e:
                self.logger.error(f"Async insert failed: {e}")
    
    def close(self):
        """Fermeture propre de la connexion"""
        self.cluster.shutdown()
1.2 GÃ©nÃ©rateur de DonnÃ©es (data_generator.py)
pythonimport random
from datetime import datetime, timedelta
import pandas as pd

class FleetDataGenerator:
    def __init__(self, num_devices=10, fleet_id='fleet-001'):
        self.num_devices = num_devices
        self.fleet_id = fleet_id
        self.devices = [f'device-{i:03d}' for i in range(num_devices)]
    
    def generate_telemetry_batch(self, num_records=100):
        """
        GÃ©nÃ¨re un batch de donnÃ©es de tÃ©lÃ©mÃ©trie
        
        Returns:
            List de dicts avec les donnÃ©es simulÃ©es
        """
        records = []
        base_time = datetime.now()
        
        for i in range(num_records):
            device_id = random.choice(self.devices)
            timestamp = base_time - timedelta(seconds=i*10)
            
            record = {
                'device_id': device_id,
                'fleet_id': self.fleet_id,
                'timestamp': timestamp,
                'lat': 48.8566 + random.uniform(-0.1, 0.1),
                'lon': 2.3522 + random.uniform(-0.1, 0.1),
                'speed_kmh': random.uniform(0, 120),
                'battery_pct': random.randint(20, 100),
                'temp_c': random.uniform(15, 30),
                'zone': random.choice(['zone_a', 'zone_b', 'zone_c'])
            }
            records.append(record)
        
        return records
    
    def generate_alert(self, device_id, severity='HIGH'):
        """GÃ©nÃ¨re une alerte pour un device"""
        return {
            'fleet_id': self.fleet_id,
            'device_id': device_id,
            'day': date.today(),
            'severity': severity,
            'timestamp': datetime.now(),
            'alert_type': random.choice(['battery_low', 'speed_limit', 'temperature']),
            'message': f'Alert for {device_id}'
        }
1.3 Pipeline Principal (etl_pipeline.py)
pythonimport time
import logging
from data_generator import FleetDataGenerator
from cassandra_writer import CassandraWriter

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ETLPipeline:
    def __init__(self):
        self.generator = FleetDataGenerator(num_devices=50)
        self.writer = CassandraWriter(
            contact_points=['cassandra-node1', 'cassandra-node2'],
            keyspace='fleet_iot',
            datacenter='dc1'
        )
    
    def run_batch_load(self, num_batches=10, batch_size=100):
        """
        Chargement par batch (historique)
        """
        logger.info("DÃ©marrage du chargement batch...")
        
        for i in range(num_batches):
            # GÃ©nÃ©ration des donnÃ©es
            records = self.generator.generate_telemetry_batch(batch_size)
            
            # Ã‰criture dans Cassandra
            start_time = time.time()
            self.writer.write_telemetry_batch(records)
            duration = time.time() - start_time
            
            logger.info(f"Batch {i+1}/{num_batches} - {batch_size} records en {duration:.2f}s")
            time.sleep(1)  # Rate limiting
    
    def run_streaming_simulation(self, duration_seconds=300):
        """
        Simulation de streaming temps rÃ©el
        """
        logger.info("DÃ©marrage du streaming simulation...")
        
        start_time = time.time()
        while time.time() - start_time < duration_seconds:
            # GÃ©nÃ©ration d'1 record par device
            records = self.generator.generate_telemetry_batch(10)
            
            # Ã‰criture asynchrone
            self.writer.write_telemetry_async(records)
            
            time.sleep(5)  # FrÃ©quence d'envoi: 5 secondes
    
    def cleanup(self):
        self.writer.close()

if __name__ == '__main__':
    pipeline = ETLPipeline()
    
    try:
        # Phase 1: Chargement historique
        pipeline.run_batch_load(num_batches=50, batch_size=200)
        
        # Phase 2: Simulation streaming
        pipeline.run_streaming_simulation(duration_seconds=600)
    
    finally:
        pipeline.cleanup()
```

### Livrables Volet 1

- [ ] `cassandra_writer.py` avec gestion MDC
- [ ] `data_generator.py` avec donnÃ©es rÃ©alistes
- [ ] `etl_pipeline.py` avec modes batch et streaming
- [ ] `requirements.txt` avec dÃ©pendances
- [ ] Dockerfile pour l'ETL
- [ ] Tests unitaires pour les composants critiques

---

## VOLET 2 : Dashboard Analytics

### Architecture du Dashboard
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DASHBOARD ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Real-Time View  â”‚     â”‚  Analytics View  â”‚     â”‚   Alerts View    â”‚
â”‚                  â”‚     â”‚                  â”‚     â”‚                  â”‚
â”‚ - Map devices    â”‚     â”‚ - Fleet stats    â”‚     â”‚ - Critical       â”‚
â”‚ - Latest status  â”‚     â”‚ - Trends charts  â”‚     â”‚ - Warnings       â”‚
â”‚ - Live metrics   â”‚     â”‚ - Comparisons    â”‚     â”‚ - Timeline       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Cassandra Client     â”‚
                      â”‚  - Query optimizer    â”‚
                      â”‚  - Caching layer      â”‚
                      â”‚  - Pagination         â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   Cassandra Cluster   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Composants Ã  ImplÃ©menter
2.1 Client Cassandra (utils/cassandra_client.py)
pythonfrom cassandra.cluster import Cluster
from cassandra.query import SimpleStatement
from datetime import datetime, date
import pandas as pd

class DashboardCassandraClient:
    def __init__(self, contact_points, keyspace):
        self.cluster = Cluster(contact_points)
        self.session = self.cluster.connect(keyspace)
    
    def get_latest_telemetry(self, device_id):
        """
        RÃ©cupÃ¨re l'Ã©tat actuel d'un device (O(1) lookup)
        """
        query = """
            SELECT * FROM latest_telemetry_by_device 
            WHERE device_id = %s
        """
        result = self.session.execute(query, (device_id,))
        return result.one()._asdict() if result else None
    
    def get_device_history(self, device_id, day, limit=100):
        """
        RÃ©cupÃ¨re l'historique journalier d'un device
        """
        query = """
            SELECT ts, lat, lon, speed_kmh, battery_pct, temp_c
            FROM telemetry_by_device_day
            WHERE device_id = %s AND day = %s
            LIMIT %s
        """
        rows = self.session.execute(query, (device_id, day, limit))
        return pd.DataFrame(list(rows))
    
    def get_fleet_alerts(self, fleet_id, day, severity=None):
        """
        RÃ©cupÃ¨re les alertes d'une flotte
        """
        if severity:
            query = """
                SELECT * FROM alerts_by_fleet_day
                WHERE fleet_id = %s AND day = %s AND severity = %s
            """
            params = (fleet_id, day, severity)
        else:
            # Note: NÃ©cessite une table supplÃ©mentaire sans severity dans PK
            # ou scan de toutes les severities
            severities = ['HIGH', 'MEDIUM', 'LOW']
            all_alerts = []
            for sev in severities:
                query = """
                    SELECT * FROM alerts_by_fleet_day
                    WHERE fleet_id = %s AND day = %s AND severity = %s
                """
                rows = self.session.execute(query, (fleet_id, day, sev))
                all_alerts.extend(list(rows))
            return pd.DataFrame(all_alerts)
        
        rows = self.session.execute(query, params)
        return pd.DataFrame(list(rows))
    
    def get_fleet_analytics(self, fleet_id, day):
        """
        RÃ©cupÃ¨re les analytics agrÃ©gÃ©es
        """
        query = """
            SELECT * FROM fleet_analytics_by_day
            WHERE fleet_id = %s AND day = %s
        """
        rows = self.session.execute(query, (fleet_id, day))
        return pd.DataFrame(list(rows))
    
    def get_all_devices_latest(self, fleet_id):
        """
        RÃ©cupÃ¨re l'Ã©tat actuel de tous les devices d'une flotte
        
        Note: NÃ©cessite d'abord lister les devices, puis requÃªter chacun
        """
        # Ã‰tape 1: Lister les devices
        query1 = """
            SELECT device_id FROM devices_by_fleet
            WHERE fleet_id = %s
        """
        device_rows = self.session.execute(query1, (fleet_id,))
        device_ids = [row.device_id for row in device_rows]
        
        # Ã‰tape 2: RÃ©cupÃ©rer l'Ã©tat de chaque device
        all_states = []
        for device_id in device_ids:
            state = self.get_latest_telemetry(device_id)
            if state:
                all_states.append(state)
        
        return pd.DataFrame(all_states)
    
    def close(self):
        self.cluster.shutdown()
2.2 Application Streamlit (app.py)
pythonimport streamlit as st
from datetime import date, timedelta
import plotly.express as px
import plotly.graph_objects as go
from utils.cassandra_client import DashboardCassandraClient

# Configuration
st.set_page_config(
    page_title="Fleet Tracing Dashboard",
    page_icon="ğŸš›",
    layout="wide"
)

# Connexion Cassandra
@st.cache_resource
def get_cassandra_client():
    return DashboardCassandraClient(
        contact_points=['cassandra-node1'],
        keyspace='fleet_iot'
    )

client = get_cassandra_client()

# Sidebar
st.sidebar.title("ğŸš› Fleet Tracing")
fleet_id = st.sidebar.text_input("Fleet ID", "fleet-001")
selected_date = st.sidebar.date_input("Date", date.today())

# Navigation
page = st.sidebar.radio("Navigation", ["Real-Time", "Analytics", "Alerts"])

# PAGE 1: Real-Time Monitoring
if page == "Real-Time":
    st.title("ğŸ“ Real-Time Fleet Monitoring")
    
    # RÃ©cupÃ©ration des donnÃ©es
    devices_data = client.get_all_devices_latest(fleet_id)
    
    if not devices_data.empty:
        # MÃ©triques globales
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Active Devices", len(devices_data))
        col2.metric("Avg Speed", f"{devices_data['speed_kmh'].mean():.1f} km/h")
        col3.metric("Avg Battery", f"{devices_data['battery_pct'].mean():.0f}%")
        col4.metric("Avg Temp", f"{devices_data['temp_c'].mean():.1f}Â°C")
        
        # Carte des devices
        st.subheader("Fleet Map")
        fig = px.scatter_mapbox(
            devices_data,
            lat='lat',
            lon='lon',
            hover_name='device_id',
            hover_data=['speed_kmh', 'battery_pct'],
            color='speed_kmh',
            size='speed_kmh',
            color_continuous_scale='Viridis',
            zoom=10,
            height=500
        )
        fig.update_layout(mapbox_style="open-street-map")
        st.plotly_chart(fig, use_container_width=True)
        
        # Table des devices
        st.subheader("Devices Status")
        st.dataframe(devices_data, use_container_width=True)
    else:
        st.warning("No devices found for this fleet")

# PAGE 2: Analytics
elif page == "Analytics":
    st.title("ğŸ“Š Fleet Analytics")
    
    # SÃ©lection du device
    device_id = st.selectbox("Select Device", [f"device-{i:03d}" for i in range(10)])
    
    # RÃ©cupÃ©ration de l'historique
    history_df = client.get_device_history(device_id, selected_date, limit=500)
    
    if not history_df.empty:
        # Graphique de vitesse
        fig_speed = px.line(
            history_df, 
            x='ts', 
            y='speed_kmh',
            title='Speed Evolution',
            labels={'speed_kmh': 'Speed (km/h)', 'ts': 'Time'}
        )
        st.plotly_chart(fig_speed, use_container_width=True)
        
        # Graphique de batterie
        fig_battery = px.line(
            history_df,
            x='ts',
            y='battery_pct',
            title='Battery Level',
            labels={'battery_pct': 'Battery (%)', 'ts': 'Time'}
        )
        st.plotly_chart(fig_battery, use_container_width=True)
        
        # Statistiques
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Max Speed", f"{history_df['speed_kmh'].max():.1f} km/h")
            st.metric("Min Battery", f"{history_df['battery_pct'].min()}%")
        with col2:
            st.metric("Avg Speed", f"{history_df['speed_kmh'].mean():.1f} km/h")
            st.metric("Avg Temp", f"{history_df['temp_c'].mean():.1f}Â°C")
    else:
        st.info("No data available for this device on this date")

# PAGE 3: Alerts
elif page == "Alerts":
    st.title("âš ï¸ Fleet Alerts")
    
    # Filtres
    severity_filter = st.selectbox("Severity", ["ALL", "HIGH", "MEDIUM", "LOW"])
    
    # RÃ©cupÃ©ration des alertes
    if severity_filter == "ALL":
        alerts_df = client.get_fleet_alerts(fleet_id, selected_date)
    else:
        alerts_df = client.get_fleet_alerts(fleet_id, selected_date, severity_filter)
    
    if not alerts_df.empty:
        # Compteurs par sÃ©vÃ©ritÃ©
        severity_counts = alerts_df['severity'].value_counts()
        col1, col2, col3 = st.columns(3)
        col1.metric("ğŸ”´ HIGH", severity_counts.get('HIGH', 0))
        col2.metric("ğŸŸ¡ MEDIUM", severity_counts.get('MEDIUM', 0))
        col3.metric("ğŸŸ¢ LOW", severity_counts.get('LOW', 0))
        
        # Timeline des alertes
        fig = px.scatter(
            alerts_df,
            x='ts',
            y='device_id',
            color='severity',
            size='severity',
            hover_data=['alert_type', 'message'],
            title='Alerts Timeline',
            color_discrete_map={'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'green'}
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Table des alertes
        st.dataframe(alerts_df.sort_values('ts', ascending=False), use_container_width=True)
    else:
        st.success("No alerts for this date ğŸ‰")

# Footer
st.sidebar.markdown("---")
st.sidebar.info("Dashboard v1.0 - Cassandra MDC Backend")
Livrables Volet 2

 app.py avec 3 pages fonctionnelles
 cassandra_client.py avec requÃªtes optimisÃ©es
 Visualisations interactives (Plotly/Matplotlib)
 Gestion de la pagination pour grandes donnÃ©es
 Dockerfile pour le dashboard
 Documentation des requÃªtes CQL utilisÃ©es


VOLET 3 : DÃ©ploiement Docker Compose
docker-compose.yml
yamlversion: '3.8'

services:
  # ==================== CASSANDRA MDC CLUSTER ====================
  
  # Datacenter 1 - Node 1
  cassandra-dc1-node1:
    image: cassandra:4.1
    container_name: cassandra-dc1-node1
    hostname: cassandra-dc1-node1
    networks:
      - fleet-network
    ports:
      - "9042:9042"
    environment:
      - CASSANDRA_CLUSTER_NAME=FleetCluster
      - CASSANDRA_DC=dc1
      - CASSANDRA_RACK=rack1
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_SEEDS=cassandra-dc1-node1,cassandra-dc2-node1
    volumes:
      - cassandra-dc1-node1-data:/var/lib/cassandra
      - ./cassandra/init-scripts:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
      interval: 30Continuer23:21s
timeout: 10s
retries: 5
Datacenter 1 - Node 2
cassandra-dc1-node2:
image: cassandra:4.1
container_name: cassandra-dc1-node2
hostname: cassandra-dc1-node2
networks:
- fleet-network
environment:
- CASSANDRA_CLUSTER_NAME=FleetCluster
- CASSANDRA_DC=dc1
- CASSANDRA_RACK=rack1
- CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
- CASSANDRA_SEEDS=cassandra-dc1-node1,cassandra-dc2-node1
volumes:
- cassandra-dc1-node2-data:/var/lib/cassandra
depends_on:
cassandra-dc1-node1:
condition: service_healthy
healthcheck:
test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
interval: 30s
timeout: 10s
retries: 5
Datacenter 2 - Node 1
cassandra-dc2-node1:
image: cassandra:4.1
container_name: cassandra-dc2-node1
hostname: cassandra-dc2-node1
networks:
- fleet-network
environment:
- CASSANDRA_CLUSTER_NAME=FleetCluster
- CASSANDRA_DC=dc2
- CASSANDRA_RACK=rack1
- CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
- CASSANDRA_SEEDS=cassandra-dc1-node1,cassandra-dc2-node1
volumes:
- cassandra-dc2-node1-data:/var/lib/cassandra
depends_on:
cassandra-dc1-node1:
condition: service_healthy
healthcheck:
test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
interval: 30s
timeout: 10s
retries: 5
Datacenter 2 - Node 2
cassandra-dc2-node2:
image: cassandra:4.1
container_name: cassandra-dc2-node2
hostname: cassandra-dc2-node2
networks:
- fleet-network
environment:
- CASSANDRA_CLUSTER_NAME=FleetCluster
- CASSANDRA_DC=dc2
- CASSANDRA_RACK=rack1
- CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
- CASSANDRA_SEEDS=cassandra-dc1-node1,cassandra-dc2-node1
volumes:
- cassandra-dc2-node2-data:/var/lib/cassandra
depends_on:
cassandra-dc2-node1:
condition: service_healthy
healthcheck:
test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
interval: 30s
timeout: 10s
retries: 5
==================== ETL SERVICE ====================
etl-pipeline:
build:
context: ./etl
dockerfile: Dockerfile
container_name: fleet-etl
networks:
- fleet-network
environment:
- CASSANDRA_HOSTS=cassandra-dc1-node1,cassandra-dc1-node2
- CASSANDRA_KEYSPACE=fleet_iot
- CASSANDRA_DC=dc1
- ETL_MODE=streaming  # batch | streaming
- LOG_LEVEL=INFO
depends_on:
cassandra-dc1-node1:
condition: service_healthy
cassandra-dc1-node2:
condition: service_healthy
restart: unless-stopped
volumes:
- ./data:/app/data
==================== DASHBOARD SERVICE ====================
dashboard:
build:
context: ./dashboard
dockerfile: Dockerfile
container_name: fleet-dashboard
networks:
- fleet-network
ports:
- "8501:8501"
environment:
- CASSANDRA_HOSTS=cassandra-dc1-node1
- CASSANDRA_KEYSPACE=fleet_iot
depends_on:
cassandra-dc1-node1:
condition: service_healthy
etl-pipeline:
condition: service_started
restart: unless-stopped
healthcheck:
test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
interval: 30s
timeout: 10s
retries: 3
==================== CASSANDRA WEB UI (Optional) ====================
cassandra-web:
image: ipushc/cassandra-web
container_name: cassandra-web-ui
networks:
- fleet-network
ports:
- "8889:3000"
environment:
- CASSANDRA_HOST=cassandra-dc1-node1
- CASSANDRA_PORT=9042
- CASSANDRA_USER=cassandra
- CASSANDRA_PASSWORD=cassandra
depends_on:
cassandra-dc1-node1:
condition: service_healthy
networks:
fleet-network:
driver: bridge
volumes:
cassandra-dc1-node1-data:
cassandra-dc1-node2-data:
cassandra-dc2-node1-data:
cassandra-dc2-node2-data:

### Scripts d'Initialisation

#### `cassandra/init-scripts/01-create-schema.cql`
```sql
-- CrÃ©ation du keyspace avec rÃ©plication MDC
CREATE KEYSPACE IF NOT EXISTS fleet_iot
WITH REPLICATION = {
  'class': 'NetworkTopologyStrategy',
  'dc1': 2,
  'dc2': 2
}
AND durable_writes = true;

USE fleet_iot;

-- Table 1: Devices par flotte
CREATE TABLE IF NOT EXISTS devices_by_fleet (
  fleet_id text,
  device_id text,
  model text,
  activated_at timestamp,
  status text,
  PRIMARY KEY ((fleet_id), device_id)
) WITH COMMENT = 'Liste des devices par flotte';

-- Table 2: Ã‰tat actuel des devices
CREATE TABLE IF NOT EXISTS latest_telemetry_by_device (
  device_id text PRIMARY KEY,
  last_ts timestamp,
  lat double,
  lon double,
  speed_kmh double,
  battery_pct int,
  temp_c double
) WITH COMMENT = 'Dernier Ã©tat connu de chaque device';

-- Table 3: Historique journalier
CREATE TABLE IF NOT EXISTS telemetry_by_device_day (
  device_id text,
  day date,
  ts timestamp,
  lat double,
  lon double,
  speed_kmh double,
  battery_pct int,
  temp_c double,
  zone text,
  PRIMARY KEY ((device_id, day), ts)
) WITH CLUSTERING ORDER BY (ts DESC)
  AND COMMENT = 'Historique de tÃ©lÃ©mÃ©trie avec bucketing journalier'
  AND default_time_to_live = 2592000;  -- 30 jours

-- Table 4: Alertes par flotte/jour/sÃ©vÃ©ritÃ©
CREATE TABLE IF NOT EXISTS alerts_by_fleet_day (
  fleet_id text,
  day date,
  severity text,
  ts timestamp,
  device_id text,
  alert_type text,
  message text,
  resolved boolean,
  PRIMARY KEY ((fleet_id, day, severity), ts, device_id)
) WITH CLUSTERING ORDER BY (ts DESC)
  AND COMMENT = 'Alertes avec filtrage par sÃ©vÃ©ritÃ©';

-- Table 5: Analytics agrÃ©gÃ©es
CREATE TABLE IF NOT EXISTS fleet_analytics_by_day (
  fleet_id text,
  day date,
  hour int,
  total_distance_km double,
  avg_speed_kmh double,
  alerts_count int,
  active_devices_count int,
  PRIMARY KEY ((fleet_id, day), hour)
) WITH COMMENT = 'MÃ©triques agrÃ©gÃ©es par heure';

-- Insertion de donnÃ©es de test
INSERT INTO devices_by_fleet (fleet_id, device_id, model, activated_at, status)
VALUES ('fleet-001', 'device-001', 'GPS-X200', toTimestamp(now()), 'active');

INSERT INTO devices_by_fleet (fleet_id, device_id, model, activated_at, status)
VALUES ('fleet-001', 'device-002', 'GPS-X200', toTimestamp(now()), 'active');
```

### Dockerfiles

#### `etl/Dockerfile`
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Installation des dÃ©pendances systÃ¨me
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copie des requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copie du code
COPY . .

# Script de dÃ©marrage
CMD ["python", "etl_pipeline.py"]
```

#### `etl/requirements.txt`
```txt
cassandra-driver==3.28.0
pandas==2.1.0
python-dateutil==2.8.2
```

#### `dashboard/Dockerfile`
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Installation des dÃ©pendances systÃ¨me
RUN apt-get update && apt-get install -y \
    gcc \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copie des requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copie du code
COPY . .

# Exposition du port Streamlit
EXPOSE 8501

# Healthcheck
HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health

# DÃ©marrage de Streamlit
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

#### `dashboard/requirements.txt`
```txt
streamlit==1.28.0
cassandra-driver==3.28.0
pandas==2.1.0
plotly==5.17.0
```

---

## Travail Ã  RÃ©aliser

### Phase 1 : Configuration de l'environnement (1h)

1. **Initialiser le projet**
   - CrÃ©er la structure de dossiers
   - Configurer `docker-compose.yml`
   - CrÃ©er les scripts d'initialisation Cassandra

2. **DÃ©marrer le cluster**
```bash
   docker compose up -d
```

3. **VÃ©rifier le cluster MDC**
```bash
   docker exec -it cassandra-dc1-node1 nodetool status
```
   
   RÃ©sultat attendu :
Datacenter: dc1
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address     Load       Tokens  Owns    Host ID   Rack
UN  172.18.0.2  X KB       256     50.0%   xxx-xxx   rack1
UN  172.18.0.3  X KB       256     50.0%   xxx-xxx   rack1
Datacenter: dc2
UN  172.18.0.4  X KB       256     50.0%   xxx-xxx   rack1
UN  172.18.0.5  X KB       256     50.0%   xxx-xxx   rack1

**Checkpoint âœ…** : Tous les services sont "healthy"

---

### Phase 2 : ImplÃ©mentation de l'ETL (3h)

#### TÃ¢che 2.1 : Writer Cassandra

- [ ] ImplÃ©menter `CassandraWriter` avec:
  - Connexion au cluster MDC
  - Prepared statements
  - MÃ©thode batch
  - MÃ©thode asynchrone
  - Gestion d'erreurs

#### TÃ¢che 2.2 : GÃ©nÃ©rateur de donnÃ©es

- [ ] CrÃ©er `FleetDataGenerator` qui gÃ©nÃ¨re:
  - DonnÃ©es de tÃ©lÃ©mÃ©trie rÃ©alistes
  - Alertes basÃ©es sur seuils
  - Distribution temporelle cohÃ©rente

#### TÃ¢che 2.3 : Pipeline ETL

- [ ] ImplÃ©menter les modes:
  - **Batch** : Chargement historique (10k+ records)
  - **Streaming** : Simulation temps rÃ©el (1 record/5s)
- [ ] Ajouter monitoring:
  - Latence d'insertion
  - Throughput (records/s)
  - Taux d'erreur

#### TÃ¢che 2.4 : Tests de charge

- [ ] Benchmark du pipeline:
  - Mesurer le throughput maximum
  - Identifier les goulots d'Ã©tranglement
  - Tester la rÃ©silience (kill un nÅ“ud)

**Livrables** :
- Code ETL complet et fonctionnel
- Rapport de performance (latence P50/P95/P99)
- Log d'exÃ©cution avec 50k+ records insÃ©rÃ©s

---

### Phase 3 : DÃ©veloppement du Dashboard (3h)

#### TÃ¢che 3.1 : Client Cassandra

- [ ] ImplÃ©menter toutes les mÃ©thodes de lecture
- [ ] Ajouter un systÃ¨me de cache (optionnel)
- [ ] Optimiser les requÃªtes multi-partitions

#### TÃ¢che 3.2 : Page Real-Time

- [ ] Carte interactive avec position des devices
- [ ] MÃ©triques en temps rÃ©el (vitesse, batterie, etc.)
- [ ] Tableau de bord avec filtres

#### TÃ¢che 3.3 : Page Analytics

- [ ] Graphiques temporels (vitesse, tempÃ©rature)
- [ ] Comparaisons entre devices
- [ ] Statistiques agrÃ©gÃ©es

#### TÃ¢che 3.4 : Page Alerts

- [ ] Liste des alertes avec filtres
- [ ] Timeline visuelle
- [ ] Compteurs par sÃ©vÃ©ritÃ©

**Livrables** :
- Dashboard fonctionnel accessible sur http://localhost:8501
- Captures d'Ã©cran des 3 pages
- Documentation des requÃªtes CQL utilisÃ©es

---

### Phase 4 : Optimisations et Tests (2h)

#### TÃ¢che 4.1 : Performance tuning

- [ ] Analyser les slow queries avec `TRACING ON`
- [ ] Optimiser les requÃªtes problÃ©matiques
- [ ] ImplÃ©menter pagination pour grandes datasets

#### TÃ¢che 4.2 : Tests de rÃ©silience

- [ ] Tester la tolÃ©rance aux pannes:
```bash
  # ArrÃªter un nÅ“ud
  docker stop cassandra-dc1-node2
  
  # VÃ©rifier que l'ETL et le dashboard fonctionnent toujours
  
  # RedÃ©marrer le nÅ“ud
  docker start cassandra-dc1-node2
```

#### TÃ¢che 4.3 : Documentation

- [ ] README.md avec instructions de dÃ©ploiement
- [ ] SchÃ©ma d'architecture (diagrammes)
- [ ] Guide d'utilisation du dashboard
- [ ] Rapport d'analyse des performances

**Livrables** :
- Documentation complÃ¨te
- Rapport de tests de rÃ©silience
- Recommandations d'amÃ©lioration

---

## CritÃ¨res d'Ã‰valuation

| CritÃ¨re | Points | DÃ©tails |
|---------|--------|---------|
| **ETL Pipeline** | 30% | FonctionnalitÃ©, performance, gestion d'erreurs |
| **Dashboard** | 25% | UI/UX, visualisations, requÃªtes optimisÃ©es |
| **ModÃ¨le Cassandra** | 20% | SchÃ©ma query-first, absence d'anti-patterns |
| **DÃ©ploiement Docker** | 15% | Configuration MDC, orchestration, healthchecks |
| **Tests & RÃ©silience** | 10% | Tests de charge, tolÃ©rance aux pannes |

---

## Bonus (Points supplÃ©mentaires)

- [ ] Ajouter une API REST (FastAPI) entre Cassandra et le dashboard
- [ ] ImplÃ©menter un systÃ¨me de cache (Redis)
- [ ] CrÃ©er des alertes en temps rÃ©el (Kafka + Consumer)
- [ ] Ajouter des tests unitaires (pytest)
- [ ] ImplÃ©menter monitoring avec Prometheus + Grafana
- [ ] GÃ©o-fencing: alertes automatiques si device sort d'une zone

---

## Ressources

### Documentation Officielle
- [Cassandra Documentation](https://cassandra.apache.org/doc/latest/)
- [Python Driver](https://docs.datastax.com/en/developer/python-driver/latest/)
- [Streamlit Docs](https://docs.streamlit.io/)

### Tutoriels RecommandÃ©s
- [Cassandra Data Modeling](https://www.datastax.com/learn/data-modeling-by-example)
- [Multi-Datacenter Replication](https://cassandra.apache.org/doc/latest/architecture/dynamo.html#multi-datacenter-replication)

---

## Planning SuggÃ©rÃ©

| Jour | ActivitÃ©s |
|------|-----------|
| J1 | Setup environnement + ETL basique |
| J2 | ETL avancÃ© + Tests de charge |
| J3 | Dashboard Real-Time + Analytics |
| J4 | Alerts + Optimisations + Tests rÃ©silience |
| J5 | Documentation + PrÃ©sentation |

---

## Rendu Final

### Structure du livrable
fleet-tracing-project.zip
â”œâ”€â”€ README.md
â”œâ”€â”€ RAPPORT.pdf
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ cassandra/
â”œâ”€â”€ etl/
â”œâ”€â”€ dashboard/
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ realtime-page.png
â”‚   â”œâ”€â”€ analytics-page.png
â”‚   â””â”€â”€ alerts-page.png
â””â”€â”€ docs/
â”œâ”€â”€ architecture.png
â”œâ”€â”€ performance-report.pdf
â””â”€â”€ user-guide.md

### PrÃ©sentation (15 min)

1. **DÃ©monstration live** (7 min)
   - DÃ©marrage du systÃ¨me
   - ETL en action
   - Navigation dans le dashboard

2. **Architecture technique** (5 min)
   - SchÃ©ma du systÃ¨me
   - Choix de modÃ©lisation Cassandra
   - StratÃ©gies d'optimisation

3. **RÃ©sultats & Perspectives** (3 min)
   - MÃ©triques de performance
   - DÃ©fis rencontrÃ©s
   - AmÃ©liorations futures

---

**Bon courage ! ğŸš€**

*Date de rendu : [Ã€ dÃ©finir]*  
*Contact : [Votre email]*